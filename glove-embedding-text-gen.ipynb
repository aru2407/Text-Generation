{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install keras_nlp","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-18T04:26:04.517892Z","iopub.execute_input":"2023-08-18T04:26:04.518280Z","iopub.status.idle":"2023-08-18T04:26:20.106188Z","shell.execute_reply.started":"2023-08-18T04:26:04.518248Z","shell.execute_reply":"2023-08-18T04:26:20.105021Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting keras_nlp\n  Downloading keras_nlp-0.6.1-py3-none-any.whl (573 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m573.5/573.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting keras-core (from keras_nlp)\n  Downloading keras_core-0.1.4-py3-none-any.whl (880 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.1/880.1 kB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (1.23.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (2023.6.3)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (13.4.2)\nRequirement already satisfied: tensorflow-text in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (2.12.1)\nCollecting namex (from keras-core->keras_nlp)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-core->keras_nlp) (3.9.0)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras-core->keras_nlp) (0.1.8)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->keras_nlp) (3.0.9)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras_nlp) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras_nlp) (2.15.1)\nRequirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras_nlp) (0.12.0)\nRequirement already satisfied: tensorflow<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras_nlp) (2.12.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.51.1)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.4.13)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (16.0.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.3.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (4.6.3)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.31.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.40.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.2.0)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.11.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.4.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.3.6)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.2.2)\nInstalling collected packages: namex, keras-core, keras_nlp\nSuccessfully installed keras-core-0.1.4 keras_nlp-0.6.1 namex-0.0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport numpy as np\nimport keras\nimport random\nimport os\nfrom keras.layers import Dense, Input, TextVectorization, Embedding, Bidirectional,LSTM\nfrom keras.models import Sequential\nfrom keras_nlp.layers import TransformerDecoder, TokenAndPositionEmbedding\nfrom keras_nlp.metrics import Perplexity\nfrom keras.callbacks import ReduceLROnPlateau,EarlyStopping\nfrom nltk.metrics.distance import edit_distance","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:26:20.108908Z","iopub.execute_input":"2023-08-18T04:26:20.110368Z","iopub.status.idle":"2023-08-18T04:26:31.588551Z","shell.execute_reply.started":"2023-08-18T04:26:20.110331Z","shell.execute_reply":"2023-08-18T04:26:31.587433Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"Using TensorFlow backend\n","output_type":"stream"}]},{"cell_type":"code","source":"raw_texts = \"\"\n#dataset: https://www.kaggle.com/datasets/tunguz/the-pg19-language-modeling-benchmark-dataset\nprint(\"Reading from file: \")\ndef read_data(file):\n    file = \"/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/\"+file+\".txt\"\n    if os.path.isfile(file) == True:\n        print(file)\n        with open(file, \"r\") as f:\n            raw_text = f.readlines()\n        return raw_text\n\nfor i in range(50):\n    num = random.randint(1,99)\n    filename = str(100)+str(num)\n    data = read_data(filename)\n    #print(type(data))\n    if type(data) == list:\n        #print(data)\n        raw_texts += str(data[100:])\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:26:31.589857Z","iopub.execute_input":"2023-08-18T04:26:31.590644Z","iopub.status.idle":"2023-08-18T04:26:32.240322Z","shell.execute_reply.started":"2023-08-18T04:26:31.590596Z","shell.execute_reply":"2023-08-18T04:26:32.239203Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Reading from file: \n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10026.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10015.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/1006.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10089.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10016.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10067.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10016.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10041.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10074.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10030.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10044.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10088.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/1003.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10088.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10013.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10077.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10041.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10083.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10027.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10079.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10094.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/1007.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10047.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10096.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10077.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10034.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10023.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/1005.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10070.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10063.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10028.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/1003.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10026.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10021.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/1002.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/1004.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10012.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10077.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"def clean_data(raw_texts):\n    raw_texts = re.sub(r'\\n','',raw_texts)\n    raw_texts = re.sub(r'\\\\n','',raw_texts)\n    raw_texts = re.sub(r\"\\'\",'',raw_texts)\n    raw_texts = re.sub(r'[*]','',raw_texts)\n    raw_texts = re.sub(r'\\\\','',raw_texts)\n    raw_texts = re.sub(r',','',raw_texts)\n    raw_texts = re.sub(r'[0-9]+','',raw_texts)\n    \n    return raw_texts\n\ndata = clean_data(raw_texts)\ndata = data.split('.')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:26:32.246449Z","iopub.execute_input":"2023-08-18T04:26:32.247172Z","iopub.status.idle":"2023-08-18T04:26:33.294285Z","shell.execute_reply.started":"2023-08-18T04:26:32.247111Z","shell.execute_reply":"2023-08-18T04:26:33.293209Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Using Glove Embeddings\nfile = \"/kaggle/input/glove-embeddings/glove.6B.100d.txt\" \nwith open(file, \"r\") as f:\n    glove_data = f.readlines()","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:26:33.299390Z","iopub.execute_input":"2023-08-18T04:26:33.301829Z","iopub.status.idle":"2023-08-18T04:26:37.630122Z","shell.execute_reply.started":"2023-08-18T04:26:33.301788Z","shell.execute_reply":"2023-08-18T04:26:37.628996Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"glove_embeddings = dict()\nfor i in range(len(glove_data)):\n    word = glove_data[i].split()[0]\n    token = glove_data[i].split()[1:]\n    token =  np.asarray([float(t) for t in token])    \n    glove_embeddings[word] = token    ","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:26:37.634006Z","iopub.execute_input":"2023-08-18T04:26:37.634670Z","iopub.status.idle":"2023-08-18T04:26:56.681134Z","shell.execute_reply.started":"2023-08-18T04:26:37.634628Z","shell.execute_reply":"2023-08-18T04:26:56.680129Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"max_tokens = 20000\nmax_length = 50\nbatch_size=128\nvectorizer = TextVectorization(output_sequence_length=max_length+1)\nvectorizer.adapt(data)","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:26:56.682554Z","iopub.execute_input":"2023-08-18T04:26:56.682915Z","iopub.status.idle":"2023-08-18T04:27:08.808386Z","shell.execute_reply.started":"2023-08-18T04:26:56.682881Z","shell.execute_reply":"2023-08-18T04:27:08.807359Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"vocab = vectorizer.get_vocabulary()\nword_index = dict(zip(vocab, range(len(vocab))))","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:27:08.809708Z","iopub.execute_input":"2023-08-18T04:27:08.810057Z","iopub.status.idle":"2023-08-18T04:27:08.977837Z","shell.execute_reply.started":"2023-08-18T04:27:08.810022Z","shell.execute_reply":"2023-08-18T04:27:08.976752Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(vocab)\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:27:08.979338Z","iopub.execute_input":"2023-08-18T04:27:08.979924Z","iopub.status.idle":"2023-08-18T04:27:08.988614Z","shell.execute_reply.started":"2023-08-18T04:27:08.979890Z","shell.execute_reply":"2023-08-18T04:27:08.987349Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"52034"},"metadata":{}}]},{"cell_type":"code","source":"embedding_dim = 100\nmax_tokens = 20000\nhits = 0\nmisses = 0\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, i in word_index.items():\n    embedding_vector = glove_embeddings.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:27:08.993277Z","iopub.execute_input":"2023-08-18T04:27:08.993905Z","iopub.status.idle":"2023-08-18T04:27:09.122243Z","shell.execute_reply.started":"2023-08-18T04:27:08.993866Z","shell.execute_reply":"2023-08-18T04:27:09.121084Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Converted 34437 words (17597 misses)\n","output_type":"stream"}]},{"cell_type":"code","source":"train = data[:int(0.85*len(data))]\ntest = data[int(0.85*len(data)):]\ntrain = tf.data.Dataset.from_tensor_slices(train).batch(batch_size)\n#train = vectorizer(train)\ntest = tf.data.Dataset.from_tensor_slices(test).batch(batch_size)\n#test = vectorizer(test)","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:27:09.123847Z","iopub.execute_input":"2023-08-18T04:27:09.124511Z","iopub.status.idle":"2023-08-18T04:27:09.483485Z","shell.execute_reply.started":"2023-08-18T04:27:09.124476Z","shell.execute_reply":"2023-08-18T04:27:09.482384Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def vectorize_dataset(text):\n    text = tf.expand_dims(text, -1)\n    tokenized_text = vectorizer(text)\n\n    x = tokenized_text[:, :-1]\n    y = tokenized_text[:, 1:]\n    return x,y\n\ntrain = train.map(vectorize_dataset)\ntrain = train.prefetch(tf.data.AUTOTUNE)\n#train = train.batch(1000)\n\ntest = test.map(vectorize_dataset)\ntest = test.prefetch(tf.data.AUTOTUNE)\n#test = test.batch(1000)","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:27:09.485002Z","iopub.execute_input":"2023-08-18T04:27:09.485372Z","iopub.status.idle":"2023-08-18T04:27:09.647674Z","shell.execute_reply.started":"2023-08-18T04:27:09.485339Z","shell.execute_reply":"2023-08-18T04:27:09.646673Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def model():\n    input_layer = Input(shape=(max_length,), dtype = tf.int32)\n    \n    embedding_layer = Embedding(\n        vocab_size,\n        embedding_dim,\n        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n        trainable=False,\n    )(input_layer)\n    \n    #embedding_layer = TokenAndPositionEmbedding(vocab_size, max_length, 256)(input_layer)\n    #bilstm_layer = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n    decoder_layer = TransformerDecoder(\n        intermediate_dim = 128,\n        num_heads = 4,\n        dropout=0.2,\n        activation=\"relu\")(embedding_layer)\n    output_layer = Dense(vocab_size,\n                         activation=\"softmax\")(decoder_layer)\n\n    model = keras.Model(inputs = input_layer, outputs = output_layer)\n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:27:09.670518Z","iopub.execute_input":"2023-08-18T04:27:09.670989Z","iopub.status.idle":"2023-08-18T04:27:09.683827Z","shell.execute_reply.started":"2023-08-18T04:27:09.670958Z","shell.execute_reply":"2023-08-18T04:27:09.682838Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model = model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:27:09.685443Z","iopub.execute_input":"2023-08-18T04:27:09.685801Z","iopub.status.idle":"2023-08-18T04:27:11.986707Z","shell.execute_reply.started":"2023-08-18T04:27:09.685768Z","shell.execute_reply":"2023-08-18T04:27:11.985909Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 50)]              0         \n                                                                 \n embedding (Embedding)       (None, 50, 100)           5203400   \n                                                                 \n transformer_decoder (Transf  (None, 50, 100)          66628     \n ormerDecoder)                                                   \n                                                                 \n dense_2 (Dense)             (None, 50, 52034)         5255434   \n                                                                 \n=================================================================\nTotal params: 10,525,462\nTrainable params: 5,322,062\nNon-trainable params: 5,203,400\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(\n        optimizer=\"adam\",\n        loss='sparse_categorical_crossentropy',\n        metrics=[Perplexity(), 'accuracy']\n    )","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:27:11.987814Z","iopub.execute_input":"2023-08-18T04:27:11.988210Z","iopub.status.idle":"2023-08-18T04:27:12.015850Z","shell.execute_reply.started":"2023-08-18T04:27:11.988175Z","shell.execute_reply":"2023-08-18T04:27:12.014976Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1)\nmodel.fit(train, validation_data = test, batch_size=2500, epochs=10, verbose=1,callbacks=[reduce_lr,early_stopping])","metadata":{"execution":{"iopub.status.busy":"2023-08-18T04:27:12.017187Z","iopub.execute_input":"2023-08-18T04:27:12.017709Z","iopub.status.idle":"2023-08-18T04:55:33.103416Z","shell.execute_reply.started":"2023-08-18T04:27:12.017674Z","shell.execute_reply":"2023-08-18T04:55:33.102352Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Epoch 1/10\n616/616 [==============================] - 218s 339ms/step - loss: 2.9009 - perplexity: 18.1898 - accuracy: 0.6647 - val_loss: 3.0043 - val_perplexity: 20.1712 - val_accuracy: 0.5842 - lr: 0.0010\nEpoch 2/10\n616/616 [==============================] - 215s 350ms/step - loss: 2.2597 - perplexity: 9.5804 - accuracy: 0.6756 - val_loss: 2.9362 - val_perplexity: 18.8446 - val_accuracy: 0.5868 - lr: 0.0010\nEpoch 3/10\n616/616 [==============================] - 217s 353ms/step - loss: 2.1786 - perplexity: 8.8338 - accuracy: 0.6770 - val_loss: 2.9096 - val_perplexity: 18.3492 - val_accuracy: 0.5880 - lr: 0.0010\nEpoch 4/10\n616/616 [==============================] - 217s 352ms/step - loss: 2.1219 - perplexity: 8.3467 - accuracy: 0.6779 - val_loss: 2.9005 - val_perplexity: 18.1830 - val_accuracy: 0.5888 - lr: 0.0010\nEpoch 5/10\n616/616 [==============================] - 233s 379ms/step - loss: 2.0799 - perplexity: 8.0033 - accuracy: 0.6784 - val_loss: 2.9079 - val_perplexity: 18.3181 - val_accuracy: 0.5879 - lr: 0.0010\nEpoch 6/10\n616/616 [==============================] - 219s 355ms/step - loss: 2.0469 - perplexity: 7.7435 - accuracy: 0.6791 - val_loss: 2.9053 - val_perplexity: 18.2702 - val_accuracy: 0.5876 - lr: 0.0010\nEpoch 7/10\n616/616 [==============================] - 219s 355ms/step - loss: 2.0230 - perplexity: 7.5608 - accuracy: 0.6797 - val_loss: 2.9030 - val_perplexity: 18.2295 - val_accuracy: 0.5885 - lr: 0.0010\nEpoch 7: early stopping\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f3b8c06baf0>"},"metadata":{}}]},{"cell_type":"code","source":"def generate_text(text,num_words):\n    #text = \"It is a wonderful\"\n    if num_words > 0:\n        for i in range(num_words):\n            text = text.lower()\n            text +=\" \"\n            seq_length = len(text.split()) - 1\n            text_vec = vectorizer([text])[:, :-1]\n            predictions = model.predict([text_vec],verbose=0)\n            logits,index = tf.math.top_k(predictions[0][seq_length], k=20, sorted=True)\n            index = np.asarray(index).astype(\"int32\")\n            for i in range(5):\n                ran = random.randint(0,len(index))\n                word = list(word_index.keys())[list(word_index.values()).index(ran)]\n                #print(word)\n                if word != '[UNK]' and word.isalpha() == True:\n                    text += word\n                    #print(word)\n                    num_words-=1\n                    #print(num_words)\n                    break\n            #print(text)\n        generate_text(text,num_words)\n    elif num_words == 0:\n         print(text)\n        \n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T05:46:17.083752Z","iopub.execute_input":"2023-08-18T05:46:17.084163Z","iopub.status.idle":"2023-08-18T05:46:17.094611Z","shell.execute_reply.started":"2023-08-18T05:46:17.084105Z","shell.execute_reply":"2023-08-18T05:46:17.093592Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"code","source":"texts = [\"It is a wonderful\",\n        \"I have not been very\",\n        \"That was not\"]\nfor text in texts:\n    print(generate_text(text,5))","metadata":{"execution":{"iopub.status.busy":"2023-08-18T05:46:19.656104Z","iopub.execute_input":"2023-08-18T05:46:19.656503Z","iopub.status.idle":"2023-08-18T05:46:20.863548Z","shell.execute_reply.started":"2023-08-18T05:46:19.656472Z","shell.execute_reply":"2023-08-18T05:46:20.862567Z"},"trusted":true},"execution_count":122,"outputs":[{"name":"stdout","text":"it is a wonderful with it he i in\nNone\ni have not been very in and that i he\nNone\nthat was not to his for as not\nNone\n","output_type":"stream"}]}]}