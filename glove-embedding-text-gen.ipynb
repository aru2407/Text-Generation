{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install keras_nlp","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-08-18T07:10:26.793943Z","iopub.execute_input":"2023-08-18T07:10:26.794217Z","iopub.status.idle":"2023-08-18T07:10:43.041688Z","shell.execute_reply.started":"2023-08-18T07:10:26.794191Z","shell.execute_reply":"2023-08-18T07:10:43.040531Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting keras_nlp\n  Downloading keras_nlp-0.6.1-py3-none-any.whl (573 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m573.5/573.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting keras-core (from keras_nlp)\n  Downloading keras_core-0.1.4-py3-none-any.whl (880 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.1/880.1 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (1.4.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (1.23.5)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (21.3)\nRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (2023.6.3)\nRequirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (13.4.2)\nRequirement already satisfied: tensorflow-text in /opt/conda/lib/python3.10/site-packages (from keras_nlp) (2.12.1)\nCollecting namex (from keras-core->keras_nlp)\n  Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.10/site-packages (from keras-core->keras_nlp) (3.9.0)\nRequirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras-core->keras_nlp) (0.1.8)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->keras_nlp) (3.0.9)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras_nlp) (2.2.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras_nlp) (2.15.1)\nRequirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras_nlp) (0.12.0)\nRequirement already satisfied: tensorflow<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-text->keras_nlp) (2.12.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras_nlp) (0.1.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.6.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (23.5.26)\nRequirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.4.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.51.1)\nRequirement already satisfied: jax>=0.3.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.4.13)\nRequirement already satisfied: keras<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.12.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (16.0.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.3.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.20.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (59.8.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.16.0)\nRequirement already satisfied: tensorboard<2.13,>=2.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.12.3)\nRequirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.12.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.3.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (4.6.3)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.31.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.40.0)\nRequirement already satisfied: ml-dtypes>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.2.0)\nRequirement already satisfied: scipy>=1.7 in /opt/conda/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.11.1)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.20.0)\nRequirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.0.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.4.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.7.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.3.6)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (4.9)\nRequirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.26.15)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2023.5.7)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (2.1.3)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<2.13,>=2.12.0->tensorflow-text->keras_nlp) (3.2.2)\nInstalling collected packages: namex, keras-core, keras_nlp\nSuccessfully installed keras-core-0.1.4 keras_nlp-0.6.1 namex-0.0.7\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\nimport tensorflow as tf\nimport numpy as np\nimport keras\nimport random\nimport os\nfrom keras.layers import Dense, Input, TextVectorization, Embedding, Bidirectional,LSTM,Dropout\nfrom keras.models import Sequential\nfrom keras_nlp.layers import TransformerDecoder, TokenAndPositionEmbedding\nfrom keras_nlp.metrics import Perplexity\nfrom keras.callbacks import ReduceLROnPlateau,EarlyStopping\nfrom nltk.metrics.distance import edit_distance","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:10:43.044964Z","iopub.execute_input":"2023-08-18T07:10:43.046098Z","iopub.status.idle":"2023-08-18T07:10:52.811057Z","shell.execute_reply.started":"2023-08-18T07:10:43.046037Z","shell.execute_reply":"2023-08-18T07:10:52.809878Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"Using TensorFlow backend\n","output_type":"stream"}]},{"cell_type":"code","source":"raw_texts = \"\"\n#dataset: https://www.kaggle.com/datasets/tunguz/the-pg19-language-modeling-benchmark-dataset\nprint(\"Reading from file: \")\ndef read_data(file):\n    file = \"/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/\"+file+\".txt\"\n    if os.path.isfile(file) == True:\n        print(file)\n        with open(file, \"r\") as f:\n            raw_text = f.readlines()\n        return raw_text\n\nfor i in range(50):\n    num = random.randint(1,99)\n    filename = str(100)+str(num)\n    data = read_data(filename)\n    #print(type(data))\n    if type(data) == list:\n        #print(data)\n        raw_texts += str(data[100:])\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:10:52.812595Z","iopub.execute_input":"2023-08-18T07:10:52.814314Z","iopub.status.idle":"2023-08-18T07:10:53.497078Z","shell.execute_reply.started":"2023-08-18T07:10:52.814272Z","shell.execute_reply":"2023-08-18T07:10:53.495935Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Reading from file: \n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/1004.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10032.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10026.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10046.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10045.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10085.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10067.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10063.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10039.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10035.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10068.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10019.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10097.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/1006.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10033.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10010.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10020.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10084.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/1007.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10027.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/1003.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10068.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10078.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10045.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10094.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10078.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10036.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10044.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10081.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10091.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/1008.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10045.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10023.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10096.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10080.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10019.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10016.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10028.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/10075.txt\n/kaggle/input/the-pg19-language-modeling-benchmark-dataset/train/train/1003.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"def clean_data(raw_texts):\n    raw_texts = re.sub(r'\\n','',raw_texts)\n    raw_texts = re.sub(r'\\\\n','',raw_texts)\n    raw_texts = re.sub(r\"\\'\",'',raw_texts)\n    raw_texts = re.sub(r'[*]','',raw_texts)\n    raw_texts = re.sub(r'\\\\','',raw_texts)\n    raw_texts = re.sub(r',','',raw_texts)\n    raw_texts = re.sub(r'[0-9]+','',raw_texts)\n    \n    return raw_texts\n\ndata = clean_data(raw_texts)\ndata = data.split('.')\n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:10:53.500383Z","iopub.execute_input":"2023-08-18T07:10:53.500974Z","iopub.status.idle":"2023-08-18T07:10:54.205160Z","shell.execute_reply.started":"2023-08-18T07:10:53.500937Z","shell.execute_reply":"2023-08-18T07:10:54.204099Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#Using Glove Embeddings\nfile = \"/kaggle/input/glove-embeddings/glove.6B.100d.txt\" \nwith open(file, \"r\") as f:\n    glove_data = f.readlines()","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:10:54.206651Z","iopub.execute_input":"2023-08-18T07:10:54.207293Z","iopub.status.idle":"2023-08-18T07:10:58.856471Z","shell.execute_reply.started":"2023-08-18T07:10:54.207252Z","shell.execute_reply":"2023-08-18T07:10:58.855475Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"glove_embeddings = dict()\nfor i in range(len(glove_data)):\n    word = glove_data[i].split()[0]\n    token = glove_data[i].split()[1:]\n    token =  np.asarray([float(t) for t in token])    \n    glove_embeddings[word] = token    ","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:10:58.857964Z","iopub.execute_input":"2023-08-18T07:10:58.858348Z","iopub.status.idle":"2023-08-18T07:11:17.224920Z","shell.execute_reply.started":"2023-08-18T07:10:58.858303Z","shell.execute_reply":"2023-08-18T07:11:17.223947Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"max_tokens = 20000\nmax_length = 50\nbatch_size=128\nvectorizer = TextVectorization(output_sequence_length=max_length+1)\nvectorizer.adapt(data)","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:11:17.226191Z","iopub.execute_input":"2023-08-18T07:11:17.226577Z","iopub.status.idle":"2023-08-18T07:11:33.544411Z","shell.execute_reply.started":"2023-08-18T07:11:17.226543Z","shell.execute_reply":"2023-08-18T07:11:33.543369Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"vocab = vectorizer.get_vocabulary()\nword_index = dict(zip(vocab, range(len(vocab))))","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:11:33.545923Z","iopub.execute_input":"2023-08-18T07:11:33.546981Z","iopub.status.idle":"2023-08-18T07:11:33.717190Z","shell.execute_reply.started":"2023-08-18T07:11:33.546944Z","shell.execute_reply":"2023-08-18T07:11:33.716151Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"vocab_size = len(vocab)\nvocab_size","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:11:33.718599Z","iopub.execute_input":"2023-08-18T07:11:33.719163Z","iopub.status.idle":"2023-08-18T07:11:33.726602Z","shell.execute_reply.started":"2023-08-18T07:11:33.719127Z","shell.execute_reply":"2023-08-18T07:11:33.725569Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"53339"},"metadata":{}}]},{"cell_type":"code","source":"embedding_dim = 100\nmax_tokens = 20000\nhits = 0\nmisses = 0\nembedding_matrix = np.zeros((vocab_size, embedding_dim))\n\nfor word, i in word_index.items():\n    embedding_vector = glove_embeddings.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:11:33.728049Z","iopub.execute_input":"2023-08-18T07:11:33.728688Z","iopub.status.idle":"2023-08-18T07:11:33.852390Z","shell.execute_reply.started":"2023-08-18T07:11:33.728654Z","shell.execute_reply":"2023-08-18T07:11:33.851308Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Converted 34505 words (18834 misses)\n","output_type":"stream"}]},{"cell_type":"code","source":"train = data[:int(0.85*len(data))]\ntest = data[int(0.85*len(data)):]\ntrain = tf.data.Dataset.from_tensor_slices(train).batch(batch_size)\n#train = vectorizer(train)\ntest = tf.data.Dataset.from_tensor_slices(test).batch(batch_size)\n#test = vectorizer(test)","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:11:33.856526Z","iopub.execute_input":"2023-08-18T07:11:33.856813Z","iopub.status.idle":"2023-08-18T07:11:34.261092Z","shell.execute_reply.started":"2023-08-18T07:11:33.856788Z","shell.execute_reply":"2023-08-18T07:11:34.260055Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def vectorize_dataset(text):\n    text = tf.expand_dims(text, -1)\n    tokenized_text = vectorizer(text)\n\n    x = tokenized_text[:, :-1]\n    y = tokenized_text[:, 1:]\n    return x,y\n\ntrain = train.map(vectorize_dataset)\ntrain = train.prefetch(tf.data.AUTOTUNE)\n#train = train.batch(1000)\n\ntest = test.map(vectorize_dataset)\ntest = test.prefetch(tf.data.AUTOTUNE)\n#test = test.batch(1000)","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:11:34.262433Z","iopub.execute_input":"2023-08-18T07:11:34.262859Z","iopub.status.idle":"2023-08-18T07:11:34.417272Z","shell.execute_reply.started":"2023-08-18T07:11:34.262825Z","shell.execute_reply":"2023-08-18T07:11:34.416268Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def model():\n    input_layer = Input(shape=(max_length,), dtype = tf.int32)\n    \n    embedding_layer = Embedding(\n        vocab_size,\n        embedding_dim,\n        embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n        trainable=False,\n    )(input_layer)\n    \n    #embedding_layer = TokenAndPositionEmbedding(vocab_size, max_length, 256)(input_layer)\n    bilstm_layer = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n    dropout_layer = Dropout(0.2)(bilstm_layer)\n    bilstm_dense_layer = Dense(128)(dropout_layer)\n    \n    decoder_layer = TransformerDecoder(\n        intermediate_dim = 128,\n        num_heads = 4,\n        dropout=0.2,\n        activation=\"relu\")(embedding_layer)\n    output_layer = Dense(vocab_size,\n                         activation=\"softmax\")(decoder_layer)\n\n    model = keras.Model(inputs = input_layer, outputs = output_layer)\n    return model\n    ","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:11:34.418705Z","iopub.execute_input":"2023-08-18T07:11:34.419869Z","iopub.status.idle":"2023-08-18T07:11:34.428628Z","shell.execute_reply.started":"2023-08-18T07:11:34.419831Z","shell.execute_reply":"2023-08-18T07:11:34.427264Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model = model()\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:11:34.430172Z","iopub.execute_input":"2023-08-18T07:11:34.430591Z","iopub.status.idle":"2023-08-18T07:11:37.261942Z","shell.execute_reply.started":"2023-08-18T07:11:34.430556Z","shell.execute_reply":"2023-08-18T07:11:37.261121Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n input_1 (InputLayer)        [(None, 50)]              0         \n                                                                 \n embedding (Embedding)       (None, 50, 100)           5333900   \n                                                                 \n transformer_decoder (Transf  (None, 50, 100)          66628     \n ormerDecoder)                                                   \n                                                                 \n dense_3 (Dense)             (None, 50, 53339)         5387239   \n                                                                 \n=================================================================\nTotal params: 10,787,767\nTrainable params: 5,453,867\nNon-trainable params: 5,333,900\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.compile(\n        optimizer=\"adam\",\n        loss='sparse_categorical_crossentropy',\n        metrics=[Perplexity(), 'accuracy']\n    )","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:11:37.263000Z","iopub.execute_input":"2023-08-18T07:11:37.263386Z","iopub.status.idle":"2023-08-18T07:11:37.290464Z","shell.execute_reply.started":"2023-08-18T07:11:37.263352Z","shell.execute_reply":"2023-08-18T07:11:37.289546Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.001)\nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1)\nmodel.fit(train, validation_data = test, epochs=10, verbose=1,callbacks=[reduce_lr,early_stopping])","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:11:37.292011Z","iopub.execute_input":"2023-08-18T07:11:37.292395Z","iopub.status.idle":"2023-08-18T07:39:22.520242Z","shell.execute_reply.started":"2023-08-18T07:11:37.292361Z","shell.execute_reply":"2023-08-18T07:39:22.519266Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/10\n727/727 [==============================] - 262s 350ms/step - loss: 2.5028 - perplexity: 12.2161 - accuracy: 0.7056 - val_loss: 1.8899 - val_perplexity: 6.6187 - val_accuracy: 0.7492 - lr: 0.0010\nEpoch 2/10\n727/727 [==============================] - 269s 370ms/step - loss: 1.9814 - perplexity: 7.2526 - accuracy: 0.7152 - val_loss: 1.8509 - val_perplexity: 6.3654 - val_accuracy: 0.7507 - lr: 0.0010\nEpoch 3/10\n727/727 [==============================] - 271s 373ms/step - loss: 1.9103 - perplexity: 6.7550 - accuracy: 0.7165 - val_loss: 1.8341 - val_perplexity: 6.2592 - val_accuracy: 0.7514 - lr: 0.0010\nEpoch 4/10\n727/727 [==============================] - 271s 372ms/step - loss: 1.8635 - perplexity: 6.4460 - accuracy: 0.7172 - val_loss: 1.8366 - val_perplexity: 6.2753 - val_accuracy: 0.7514 - lr: 0.0010\nEpoch 5/10\n727/727 [==============================] - 271s 373ms/step - loss: 1.8285 - perplexity: 6.2249 - accuracy: 0.7176 - val_loss: 1.8357 - val_perplexity: 6.2695 - val_accuracy: 0.7517 - lr: 0.0010\nEpoch 6/10\n727/727 [==============================] - 268s 369ms/step - loss: 1.8028 - perplexity: 6.0668 - accuracy: 0.7180 - val_loss: 1.8365 - val_perplexity: 6.2743 - val_accuracy: 0.7520 - lr: 0.0010\nEpoch 6: early stopping\n","output_type":"stream"},{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7d1109d72170>"},"metadata":{}}]},{"cell_type":"code","source":"def generate_text(text,num_words):\n    #text = \"It is a wonderful\"\n    if num_words > 0:\n        for i in range(num_words):\n            text = text.lower()\n            text +=\" \"\n            seq_length = len(text.split()) - 1\n            text_vec = vectorizer([text])[:, :-1]\n            predictions = model.predict([text_vec],verbose=0)\n            logits,index = tf.math.top_k(predictions[0][seq_length], k=20, sorted=True)\n            index = np.asarray(index).astype(\"int32\")\n            for i in range(5):\n                ran = random.randint(0,len(index))\n                word = list(word_index.keys())[list(word_index.values()).index(ran)]\n                #print(word)\n                if word != '[UNK]' and word.isalpha() == True:\n                    text += word\n                    #print(word)\n                    num_words-=1\n                    #print(num_words)\n                    break\n            #print(text)\n        generate_text(text,num_words)\n    elif num_words == 0:\n         print(text)\n        \n","metadata":{"execution":{"iopub.status.busy":"2023-08-18T07:39:22.521926Z","iopub.execute_input":"2023-08-18T07:39:22.522332Z","iopub.status.idle":"2023-08-18T07:39:22.533730Z","shell.execute_reply.started":"2023-08-18T07:39:22.522296Z","shell.execute_reply":"2023-08-18T07:39:22.532537Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"texts = [\"It is a wonderful\",\n        \"I have not been very\",\n        \"That was not\",\n        \"He must not have been\"]\nfor text in texts:\n    print(generate_text(text,5))","metadata":{"execution":{"iopub.status.busy":"2023-08-18T08:05:15.356434Z","iopub.execute_input":"2023-08-18T08:05:15.356831Z","iopub.status.idle":"2023-08-18T08:05:16.896736Z","shell.execute_reply.started":"2023-08-18T08:05:15.356785Z","shell.execute_reply":"2023-08-18T08:05:16.895042Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"it is a wonderful for in of to the\nNone\ni have not been very you is was had was\nNone\nthat was not had a in a of\nNone\nhe must not have been with the in as that\nNone\n","output_type":"stream"}]}]}